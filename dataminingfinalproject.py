# -*- coding: utf-8 -*-
"""DataMiningFinalProject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10cTZBIufRMNKQ4qbEqC_GiUaPmyr_XE3
"""

#Installs google drive python modules
!pip install PyDrive
!pip install regex
#Import functions to create connections between Drive and Colab. 
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
import pandas as pd
import numpy as np
import regex as re 
#Authenticate and create pydrive client, click on the link, allow Google SDK to access drive, and past code into the text box
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
downloaded = drive.CreateFile({'id':"1_wcpzL5I8LXHeTYczMVPUW4UXIN4dHXH"})   #File ID
downloaded.GetContentFile('LeagueofLegends.csv')        # Specifying file
data = pd.read_csv('LeagueofLegends.csv') #Read in the data

data.keys()

KillTimeRegex = re.compile(r'\d+\.\d+')
index = 0 
allKillTimes = []
for games in data['bKills']:
  KillFind = KillTimeRegex.findall(games)
  killTimes = []
  for nums in KillFind:
    killTimes.append(float(nums))
  allKillTimes.append(killTimes)
allKillTimes_np = np.array(allKillTimes)
allKillTimes_np_df = pd.DataFrame(allKillTimes)
allKillTimes_np_df_under10 = allKillTimes_np_df[allKillTimes_np_df < 10]
allKillTimes_np_df_under10
blueKills = []
for index, row in allKillTimes_np_df_under10.iterrows():
  numKills = 0
  for i in row: 
    if not np.isnan(i):
      numKills +=1 
  blueKills.append(numKills)
blueKillsdf = pd.DataFrame(np.array(blueKills),columns = ['BlueKills'])
KillTimeRegex = re.compile(r'\d+\.\d+')
index = 0 
allKillTimes = []
for games in data['rKills']:
  KillFind = KillTimeRegex.findall(games)
  killTimes = []
  for nums in KillFind:
    killTimes.append(float(nums))
  allKillTimes.append(killTimes)
allKillTimes_np = np.array(allKillTimes)
allKillTimes_np_df = pd.DataFrame(allKillTimes)
allKillTimes_np_df_under10 = allKillTimes_np_df[allKillTimes_np_df < 10]
allKillTimes_np_df_under10
blueKills = []
for index, row in allKillTimes_np_df_under10.iterrows():
  numKills = 0
  for i in row: 
    if not np.isnan(i):
      numKills +=1 
  blueKills.append(numKills)
redKillsdf = pd.DataFrame(np.array(blueKills),columns = ['RedKills'])

#make goldDiff 10 separate columns
newdata1 = np.zeros((data['golddiff'].size,10));
for i in range(data['golddiff'].size - 1):
  newdata1[i] = np.matrix(data['golddiff'][i])[0,0:10];

#make goldDiff 10 separate columns
newdata2 = np.zeros((data['goldblueTop'].size,10));
for i in range(data['goldblueTop'].size - 1):
  newdata2[i] = np.matrix(data['goldblueTop'][i])[0,0:10];

#make goldDiff 10 separate columns
newdata3 = np.zeros((data['goldblueJungle'].size,10));
for i in range(data['goldblueJungle'].size - 1):
  newdata3[i] = np.matrix(data['goldblueJungle'][i])[0,0:10];

#make goldDiff 10 separate columns
newdata4 = np.zeros((data['goldblueMiddle'].size,10));
for i in range(data['goldblueMiddle'].size - 1):
  newdata4[i] = np.matrix(data['goldblueMiddle'][i])[0,0:10];

#make goldDiff 10 separate columns
newdata5 = np.zeros((data['goldblueADC'].size,10));
for i in range(data['goldblueADC'].size - 1):
  newdata5[i] = np.matrix(data['goldblueADC'][i])[0,0:10];

#make goldDiff 10 separate columns
newdata6 = np.zeros((data['goldblueSupport'].size,10));
for i in range(data['goldblueSupport'].size - 1):
  newdata6[i] = np.matrix(data['goldblueSupport'][i])[0,0:10];

#make goldDiff 10 separate columns
newdata7 = np.zeros((data['goldredTop'].size,10));
for i in range(data['goldredTop'].size - 1):
  newdata7[i] = np.matrix(data['goldredTop'][i])[0,0:10];

#make goldDiff 10 separate columns
newdata8 = np.zeros((data['goldredJungle'].size,10));
for i in range(data['goldredJungle'].size - 1):
  newdata8[i] = np.matrix(data['goldredJungle'][i])[0,0:10];

#make goldDiff 10 separate columns
newdata9 = np.zeros((data['goldredMiddle'].size,10));
for i in range(data['goldredMiddle'].size - 1):
  newdata9[i] = np.matrix(data['goldredMiddle'][i])[0,0:10];

#make goldDiff 10 separate columns
newdata10 = np.zeros((data['goldredADC'].size,10));
for i in range(data['goldredADC'].size - 1):
  newdata10[i] = np.matrix(data['goldredADC'][i])[0,0:10];

#make goldDiff 10 separate columns
newdata11 = np.zeros((data['goldredSupport'].size,10));
for i in range(data['goldredSupport'].size - 1):
  newdata11[i] = np.matrix(data['goldredSupport'][i])[0,0:10];

goldDiff = pd.DataFrame(newdata1,columns = ['1 min gd', '2 min gd', '3 min gd', '4 min gd', '5 min gd', '6 min gd', '7 min gd', '8 min gd', '9 min gd', '10 min gd'])
gold1 = pd.DataFrame(newdata2,columns = ['1 min gbt', '2 min gbt', '3 min gbt', '4 min gbt', '5 min gbt', '6 min gbt', '7 min gbt', '8 min gbt', '9 min gbt', '10 min gbt'])
gold2 = pd.DataFrame(newdata3,columns = ['1 min gbj', '2 min gbj', '3 min gbj', '4 min gbj', '5 min gbj', '6 min gbj', '7 min gbj', '8 min gbj', '9 min gbj', '10 min gbj'])
gold3 = pd.DataFrame(newdata4,columns = ['1 min gbm', '2 min gbm', '3 min gbm', '4 min gbm', '5 min gbm', '6 min gbm', '7 min gbm', '8 min gbm', '9 min gbm', '10 min gbm'])
gold4 = pd.DataFrame(newdata5,columns = ['1 min gba', '2 min gba', '3 min gba', '4 min gba', '5 min gba', '6 min gba', '7 min gba', '8 min gba', '9 min gba', '10 min gba'])
gold5 = pd.DataFrame(newdata6,columns = ['1 min gbs', '2 min gbs', '3 min gbs', '4 min gbs', '5 min gbs', '6 min gbs', '7 min gbs', '8 min gbs', '9 min gbs', '10 min gbs'])
gold6 = pd.DataFrame(newdata7,columns = ['1 min grt', '2 min grt', '3 min grt', '4 min grt', '5 min grt', '6 min grt', '7 min grt', '8 min grt', '9 min grt', '10 min grt'])
gold7 = pd.DataFrame(newdata8,columns = ['1 min grj', '2 min grj', '3 min grj', '4 min grj', '5 min grj', '6 min grj', '7 min grj', '8 min grj', '9 min grj', '10 min grj'])
gold8 = pd.DataFrame(newdata9,columns = ['1 min grm', '2 min grm', '3 min grm', '4 min grm', '5 min grm', '6 min grm', '7 min grm', '8 min grm', '9 min grm', '10 min grm'])
gold9 = pd.DataFrame(newdata10,columns = ['1 min gra', '2 min gra', '3 min gra', '4 min gra', '5 min gra', '6 min gra', '7 min gra', '8 min gra', '9 min gra', '10 min gra'])
gold10 = pd.DataFrame(newdata11,columns = ['1 min grs', '2 min grs', '3 min grs', '4 min grs', '5 min grs', '6 min grs', '7 min grs', '8 min grs', '9 min grs', '10 min grs'])

LeagueData = pd.concat([blueKillsdf,redKillsdf,goldDiff,gold1,gold2,gold3,gold4,gold5,gold6,gold7,gold8,gold9,gold10,data['bResult']], axis=1, join='outer')
LeagueData

#classification
import numpy as np
import seaborn as sn
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.naive_bayes import GaussianNB as GNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA
from sklearn.tree import DecisionTreeClassifier as DT
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# custom function to plot confusion matrix
from sklearn.utils.multiclass import unique_labels
def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = classes[unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 1.5
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

np.set_printoptions(precision=2)

# importing League of Legends data

X = LeagueData.values[:,0:-1];
y = LeagueData['bResult'].values;

# whiten the data

n = 10
whiten = PCA(n_components = n, whiten = True)
X_whitened = whiten.fit_transform(X)
X = X_whitened

# try KNN with different k values

# k_list = k value list
k_list = np.arange(300,800,50)
scores = []
for k in k_list:
    clf = KNN(n_neighbors = k)
    scores.append(cross_val_score(clf, X, y, cv = 10, scoring = 'accuracy'))

scores_avg = np.mean(scores,axis=1)
score_max = max(scores_avg)
idx = list(scores_avg).index(score_max)
print("KNN scores:")

for i in range(len(scores_avg)):
    print("k = %d: %f" %(k_list[i], scores_avg[i]))
print("highest score: %f for k = %d" %(score_max, k_list[idx]))

# visualize confusion matrix for best classfier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.7)

n = k_list[idx]
clf = KNN(n_neighbors = n)
y_pred = clf.fit(X_train, y_train).predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)

df_cm = pd.DataFrame(cm, index = [i for i in "01"],
                  columns = [i for i in "01"])
ax = sn.heatmap(df_cm, annot = True, linewidths = 1, cmap = plt.cm.Blues)

# Plot normalized confusion matrix for k-NN
class_names = np.array([0,1])
plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,
                      title='Normalized confusion matrix for k-NN')

# try Gaussian Naive Bayes with different priors

# p_list = prior list
p1 = [0.5, 0.5]
p2 = [0.54, 0.46]
p3 = [0.46, 0.54]
p4 = [0.4, 0.6]
p_list = [p1, p2, p3, p4]

scores = []
for p in p_list:
    clf = GNB(priors = p)
    scores.append(cross_val_score(clf, X, y, cv = 10, scoring = 'accuracy'))

scores_avg = np.mean(scores,axis=1)
score_max = max(scores_avg)
idx = list(scores_avg).index(score_max)
print("GNB scores:")
for i in range(len(scores_avg)):
    print("%f for prior = %s" %(scores_avg[i], p_list[i]))
print("highest score: %f for prior = %s" %(score_max, p_list[idx]))

# visualize confusion matrix for best classfier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.7)

n = p_list[idx]
clf = GNB(priors = n)
y_pred = clf.fit(X_train, y_train).predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)

df_cm = pd.DataFrame(cm, index = [i for i in "01"],
                  columns = [i for i in "01"])
ax = sn.heatmap(df_cm, annot = True, linewidths = 1, cmap = plt.cm.Blues)

# Plot normalized confusion matrix for GNB
class_names = np.array([0,1])
plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,
                      title='Normalized confusion matrix for GNB')

# try random forest classifiers

# t_list = tree list (number of trees in random forest)
t_list = [25, 50, 75, 100]

scores = []
for i in t_list:
    clf = RF(n_estimators = i)
    scores.append(cross_val_score(clf, X, y, cv = 10, scoring = 'accuracy'))

scores_avg = np.mean(scores,axis=1)
score_max = max(scores_avg)
idx = list(scores_avg).index(score_max)
print("RF scores:")
for i in range(len(scores_avg)):
    print("%f for # trees = %s" %(scores_avg[i], t_list[i]))
print("highest score: %f for # trees = %s" %(score_max, t_list[idx]))

# visualize confusion matrix for best classfier

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.7)

n = t_list[idx]
clf = RF(n_estimators = n)
y_pred = clf.fit(X_train, y_train).predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)

df_cm = pd.DataFrame(cm, index = [i for i in "01"],
                  columns = [i for i in "01"])
ax = sn.heatmap(df_cm, annot = True, linewidths = 1, cmap = plt.cm.Blues)

# Plot normalized confusion matrix for RF
class_names = np.array([0,1])
plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,
                      title='Normalized confusion matrix for RF')

# try random forest classifiers

# t_list = tree list (number of trees in random forest)
t_list = [15, 25, 50]

scores = []
for i in t_list:
    clf = RF(n_estimators = i)
    scores.append(cross_val_score(clf, X, y, cv = 10, scoring = 'accuracy'))

scores_avg = np.mean(scores,axis=1)
score_max = max(scores_avg)
idx = list(scores_avg).index(score_max)
print("RF scores:")
for i in range(len(scores_avg)):
    print("%f for # trees = %s" %(scores_avg[i], t_list[i]))
print("highest score: %f for # trees = %s" %(score_max, t_list[idx]))

LeagueDataEasy = LeagueData[['BlueKills','RedKills','10 min gd','bResult']]
X = LeagueDataEasy.iloc[:,:-1]
y = LeagueDataEasy['bResult']
clf = RF(n_estimators = 500)
LeagueModel = clf.fit(X,y)
testData = np.array([[3,6,-400]])
print(LeagueModel.predict_proba(testData))
print(LeagueModel.predict(testData))

np.math.e**(-0.22564668)